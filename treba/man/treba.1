\.TH treba 1 "July 9, 2012"
.SH NAME
treba \- probabilistic finite-state automaton training and decoding
.SH SYNOPSIS
.B treba [options] [
.I OBSERVATIONS-FILE
.B ]
.SH DESCRIPTION
.B treba
is a tool for training, decoding, and calculating with weighted (probabilistic) finite state automata.  Training algorithms include Baum-Welch (EM), Viterbi training, and Baum-Welch augmented with deterministic annealing.  Training algorithms can be run multi-threaded on hardware with multiple cores/CPUs.  Forward, backward, and Viterbi decoding are supported.  Automata for training/decoding are read from a text file, or can be generated randomly or with uniform transition probabilities with different topologies (ergodic or fully connected, Bakis or left-to-right, or deterministic).  Observations used for training or decoding are read from text files.  The resulting automata, path calculations, or probability calculations are printed to standard output.   

.SH BASIC OPTIONS
The program has four main modes of operation: training (the 
.B -T
flag), decoding (the
.B -D 
flag), likelihood calculation (the
.B -L
flag), and sequence generation (the
.B -G
flag).  All modes except generation depend on an observations file and possibly a finite-state automaton file (if not initialized randomly for training). 
The observations file is a text file and is assumed to consist of whitespace-separated sequences of integers, one observation sequence on each line.  Empty lines correspond to the empty string and are acceptable.  All output is directed to the standard output: when run in training mode, the output will consist of a FSA in a text format (see below); when run in decoding mode, the output will be a string of whitespace-separated integers representing the most probable path through a given FSA, one path for each observation; when run in likelihood calculation mode, the output will be one probability for each line in the observations file.

.TP
.B \-T bw|dabw|vit|vitbw
Train a model with one of the four algorithms 
.B bw
(Baum-Welch),
.B dabw
(Baum-Welch with deterministic annealing),
.B vit
(Viterbi training), or
.B vitbw
(Viterbi training followed by Baum-Welch).
.TP
.B \-D f|b|vit(,p)
Decode (find the best path) through the automaton for each word in obervation-file using either the forward path (
.B f
)
, the backward path (
.B b
), or the Viterbi path (
.B vit
).  The optional 
.B ,p
will also print out the respective probability together with the path.  Note that forward and backward decoding chooses the most probable state for each point in time and so the path may or may not correspond to an actually valid path in the automaton.  For example, 
.B -D vit,p 
will calculate the Viterbi path and print its probability.
.TP
.B \-L f|vit
Calculate the likelihood (probability) for each observation in observation-file using forward probability, or the Viterbi probability.
.TP
.BI \-G " numsequences"
Generate
.I numsequences
random sequences from FSA.  Randomness is weighted by transition probabilities.  The sequences are output in three TAB-separated fields: (1) the sequence probability; (2) the symbol sequence itself; (3) the state sequence.

.TP
.BI \-i " input-format"
Set format of probabilities in input automata (real numbers or logs or negative logs in various bases), one of 
.B real, log10, ln, log2, nlog10, nln, nlog2.
Default is 
.B real.
.TP
.BI \-o " output-format"
Set format of probabilities related to output automata or results of decoding and likelihood calculations (real numbers or logs or negative logs in various bases), one of 
.B real, log10, ln, log2, nlog10, nln, nlog2.
Default is 
.B real.
.TP
.BI \-f " fsm-file"
Specify finite state automaton file.  Each line in the automaton file consists of one to four numbers: a four-number line 
S1 S2 A P
indicates a transition from S1 to S2 with symbol 
.I A 
and probability 
.I P
whereas a line of the format
S P
indicates the final probability at state
.I S.
Three-number and one-number lines are identical to the above, with an implicit probabibility
.I P 
of one.  The initial state is always state number 0.  The format is identical to the text formats accepted by the AT&T FSM toolkit or OpenFST, with the exception that strings are not allowed to represent symbols or states: all symbols and states need to be integers.

The following snippet illustrates a typical FSA file of two states with an alphabet size of three using real-valued probabilities:

.PP
\f(CW      0 0 0 0.25 \fR\br
\f(CW      0 0 1 0.25 \fR\br
\f(CW      0 1 0 0.2 \fR\br
\f(CW      0 1 1 0.1 \fR\br
\f(CW      0 1 2 0.1 \fR\br
\f(CW      1 0 0 0.15 \fR\br
\f(CW      1 0 1 0.15 \fR\br
\f(CW      1 1 0 0.3 \fR\br
\f(CW      1 1 1 0.1 \fR\br
\f(CW      1 1 2 0.1 \fR\br
\f(CW      0 0.1\fR\br
\f(CW      1 0.2\fR\br
.PP

.TP
.BI \-g " type"
Generate an initial automaton of type
.I type
for training.  The type is a combination of an option letter and the number of the states and the alphabet of the format
.B b|d|n#states(,#numsymbols)
Here, 
.B b 
will generate a Bakis (left-to-right) automaton, 
.B d 
a deterministic automaton, and 
.B n 
a fully connected (ergodic) nondeterministic automaton.  If
.B numsymbols 
is omitted, the necessary alphabet size will be deduced from the observation file.  For example, 
.B -n20,10
will generate an initial fully connected random automaton with 20 states and 10 symbols whereas
.B -b10
will generate a Bakis automaton with 10 states, and the alphabet size is deduced from the largest symbol in the observations file.
.TP
.BI \-u 
This option sets all initially generated automata with the 
.B -g 
option to have uniform probabilities instead of randomly assigned ones.
.TP
.BI \-h
print help and exit
.TP
.BI \-v
print version and exit

.SH TRAINING OPTIONS
.TP
.BI \-d " max-delta"
Maximum delta (change in log likelihood between training iterations) to use for convergence.  Default is 0.1.  Note that treba will output the result of training calculations so far if CTRL-C is pressed without the need to wait for convergence.
.TP
.BI \-x " maxiterations"
Maximum number of iterations in training.  Default is 100000.  Note that treba will output the result of training calculations so far if CTRL-C is pressed without the need to wait for convergence.
.TP
.BI \-p " pseudocounts"
Pseudocounts to use in Viterbi training to prevent paths of probability zero. Default value is 1.
.TP
.B \-r restarts,iterations-per-restart
Sets Baum-Welch to restart itself 
.B restarts 
times running for
.B iterations-per-restart 
iterations each restart.  After all random restarts have been run, the fsm with the best log likelihood is chosen and Baum-Welch proceeds as normal.

.TP
.B \-a betamin,betamax,alpha
Controls the parameters for deterministic annealing when run with 
.B -T dabw
setting the initial beta value to 
.B betamin,
the maximum beta value to 
.B betamax
and the multiplier 
.B alpha 
by which beta in increased each time Baum-Welch converges.  The default values are 0.02, 1.0, and 1.01.
.TP
.BI \-t " num-threads"
Number of threads to launch in Baum-Welch training.  The value 
.B num-threads 
can be optionally prefixed by
.B c 
or 
.B c/
to use 
.B c-n 
threads or 
.B c/n 
threads, where 
.B c 
is the number of logical CPUs on the system.  To use half the available processors one would issue the flag
.B -t c/2 
and likewise 
.B -t c1 
to use all but one of the available CPUs/cores. Default value is 1.
.SH EXAMPLE USAGE
.IP "treba -T bw -g n10 sentences.txt"
Reads all the sentences from sentences.txt and trains a 10-state probabilistic automaton using Baum-Welch using the default training parameters.  The initial automaton has random probabilities on its transitions and fully connected.
.IP "treba -T bw -t c/2 -g b100 sentences.txt"
Reads all the sentences from sentences.txt and trains a 100-state probabilistic automaton using Baum-Welch.  The initial automaton is left-to-right.  During training, half the available CPUs will be used. 
.IP "treba -T dabw -t 8 -f initial.fsm sentences.txt"
Reads all the sentences from sentences.txt and trains a probabilistic automaton using Baum-Welch with deterministic annealing.  The initial automaton is read from initial.fsm.  A total of 8 threads will be launched in parallel for Baum-Welch.
.IP "treba -T vit -g b25,5 -x 10 sentences.txt"
Reads all the sentences from sentences.txt and trains a 25-state probabilistic automaton with an alphabet size of 5 using Viterbi training running a maximum of 10 iterations.  The initial automaton is random and left-to-right (Bakis).
.IP "treba -L f -f myfsm.fsm sentences.txt"
Reads sentences.txt and calculates for each observation line the forward probability in the automaton in myfsm.fsm.
.IP "treba -D vit -f myfsm.fsm sentences.txt"
Reads sentences.txt and calculates for each observation line the most probable path (the Viterbi path) in the automaton myfsm.fsm.
.IP "treba -D vit,p -f myfsm.fsm sentences.txt"
Reads sentences.txt and calculates for each observation line the most probable path through the automaton in myfsm.fsm.  The probability of the path is also printed.
.IP "treba -G 100 -o log10 -f myfsm.fsm"
Generate 100 random sequences (weighted by transition probabilities) from myfsm.fsm.  Output probability scores are log10 and myfsm.fsm has real-valued transitions (default).
.IP "treba -i real -o nln -f myfsm.fsm"
No real action: myfsm.fsm is read (inputs are real-valued) and converted to negative logprobs (aka weights) with the base e and output to stdout.  This can be used to export an FSA for processing with e.g. the AT&T tools or OpenFST.  Not issuing any of the flags -T, -D or -L, will simply output the input FSA, performing a possible conversion depending on the input and output specifiers.

.SH "SEE ALSO"

de la Higuera, C. (2010). Grammatical Inference: Learning Automata and Grammars.  Cambridge University Press.

Dempster, A., N. Laird, and D. Rubin. (1977). Maximum likelihood estimation from incomplete data via the EM algorithm. Journal of the Royal Statistical Society B, 39:1\(en38.

Rabiner, L. R. (1989). A tutorial on Hidden Markov Models and selected applications in speech recognition.  Proc. of the IEEE, 77(2):257\(en286.

Rao, A. and K. Rose. (2001). Deterministically annealed design of Hidden Markov Model speech recognizers. IEEE Transactions on Speech and Audio Processing, 9(2):111\(en126.

Rose, K. (1998). Deterministic annealing for clustering, compression, classification, regression, and related optimization problems. Proc. of the IEEE, 86(11):2210\(en2239.

Ueda, N. and Nakano, R. (1998). Deterministic annealing EM algorithm. Neural Networks, 11(2):271\(en282.

Smith, N. A. and Eisner, J. (2004). Annealing techniques for unsupervised statistical language learning. In Proc. of the ACL, pages 486\(en493.

.BR fsm (1)

.SH BUGS
Many and hairy. This is academic code. Don't build Mars rovers with it.

.SH AUTHOR
.LP
Mans Hulden <mans.hulden@gmail.com>
